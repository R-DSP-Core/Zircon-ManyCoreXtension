# 1 概述

## 1.1 算力芯片的存储层级
经过调研，大多数计算任务主导的众核处理器芯片都遵循L1级存储私有，L2级存储部分共享 (注：本文中存储层级统一称为Lx Memory，Memory是Cache和SPM的总称)；另外，算力芯片的存储结构也多拥有被动缓存 (如TCM、SPM)和自调度缓存 (传统Cache)混合，某些产品中二者的比例可调。例如申威26010的计算核阵列，其L1存储分为指令和数据存储，其中L1I完全是Cache，而L1D则是混合比例可调的Cache和SPM，L2仅是指令存储，和定制化内存控制器一起进一步降低片上指令数据miss。再如英伟达GPU，物理存储拥有一个SM (Stream Multiprocessor)完全独占的指令缓存、可控比例的64KB共享内存/L1 Cache、独占的只读数据Cache和若干独占Texture Memory，片上全局共享L2 Cache。TI的C66x VLIW DSP内核总体上也使用了可配比的L1, L2存储且L1, L2均为每个内核私有，除此之外，TI还提供了可配成L2/L3的八核共享片上4MB SRAM，二者区别在于是否可被每个核心的私有L2存储缓存；其他DSP如Cadence HIFI-5s SMP采用私有L1 Cache/SPM和共享L2 Cache/SPM的架构。<br>

## 1.2 全局架构

通过对现有架构的研究，我们总决定采用类似的存储层级，总体上64个VLIW核心分为两层，第一层级有8个VLIW计算核心、DMA和共享L2 Memory，第二层级包含8个八核心簇、DMA、内存控制器控制的L3 Memory/DDR通道以及PCIe。第二层级用于对大型计算任务进行粗粒度划分，第一层级用于对粗分子任务进行细粒度划分。由于第一层级的主机有DMA和八个内核，从机只有对外接口和共享内存，远小于交叉开关的通信压力上限，故可以采用基于交叉开关的AXI4+ACE通信架构；第二层级涉及到复杂的PCIe、内存控制器及DMA和八个簇间的大规模数据搬运，故需要考虑采用基于NoC的AXI4通信协议。

## 1.3 地址编址

在TI的八核DSP处理器中，所有核心的L1P/D Memory和L2 Memory配置成SPM的部分对于其他核心都是可视的 (无论在程序的逻辑空间还是硬件的物理空间)，但TI在每一级缓存控制器加入配置寄存器实现固定地址段的内存保护特性；对于算力芯片申威26010和英伟达GPU，这二者的硬件线程共享内存对其他硬件线程不可见。究其原因是核心众多的情况下频繁的片上大流量数据交换可能会导致系统死锁和通信拥塞的发生，导致系统崩溃。

为了保证一定的灵活性，在用户态程序逻辑空间中，同一个簇内的所有内核的L1I/D Memory互相可见，而不同簇的内核之间其L1I/D Memory互相不可见，所有簇的L2 Memory对于所有簇的内核可见，逻辑地址向物理地址转换使用位宽扩充和段偏移的方式。任务分配直接采用“一簇一任务”的原则。这样做的目的是为了尽量避免在双层结构中内核直接访问互相L1I/D Memory造成的第一层级通信压力过大。粗粒度任务间同步采用L2 Memory间DMA通信实现。

虚拟地址和物理地址需要明确在特权态和用户态中内核能看到并操控的资源。对于特权态，一般在高性能嵌入式和DSP处理器中，如果是没有RTOS参与的裸机编程，只有在系统初始化过程中能配置一些寄存器如内存保护、地址重映射以及重映射后数据的搬移；在有操作系统的情况下，这些资源配置完全由操作系统调度和控制。另外，操作系统的介入资源调度也也能让内核实现任务切换。对于用户态，基本只有应用内的数据可以自由使用，程序只能按照特权态的分配地址进行访问。基于这两点观察和在上一段的讨论，在用户态，内核能够操控的资源有：本地L1D SPM, 本簇L1D SPM和所有簇的L2 SPM；而在特权态，应保证所有的资源都能被看到。

虚拟地址定为32位，物理地址定为40位，需要明确的是这里别名地址也参与虚拟和物理地址的编址。

我们用簇中核别名、簇中核真名、簇别名、簇真名、真名来代指L1 SPM核L2 SPM在不同视角的地址。
1.	真名就是所有存储占用的40位物理地址。
    2.	对簇中核别名，簇中核真名、簇别名和簇真名有：虚拟地址=物理地址，只是位宽不同，这部分的地址转换直接使用位宽扩展。
    3.	每个核心的核真名不等于簇中核真名，而是其段偏置。偏置后地址完全位于32位寻址的4GB之外。
    4.	第一层级crossbar是簇内的通信交叉开关，对应所有簇间通信有第二层级crossbar，第一层级支持一致性，第二层级不支持一致性。

簇中核别名核簇中核真名都是对于一个簇内所有的内核的L1 SPM来说的，一个簇中有八个核心，故有八个簇中核真名，一个簇中核别名。核心发出的32位簇中核别名核和本核心的簇中核真名同样可以访问直接访问到本地L1 SPM，但使用其他核心的簇中核真名第一层级AXI/ACE crossbar会译码给本簇发送给其他核心。

簇别名和簇真名是对于所有簇的L2 SPM来说的。当簇内核心使用32位簇别名和本簇的簇真名经过位宽扩充至40位后，第一层级AXI/ACE crossbar会译码给簇内L2 SPM。使用位宽扩充后的其他簇真名，第一层级crossbar会将译码转至留给第二层级crossbar的接口，第二层级AXI crossbar转至其他簇的L2 SPM。

使用真名，即L1 SPM的物理地址，双层crossbar可以准确导引至所有L1 SPM和L2 SPM，只能在初始化或者特权模式使用，且在这两种模式下不能使用簇中核别名、簇中核真名、簇别名、簇真名。对于用户模式对L1 的操控只能局限在簇内，只能使用簇中核别名和簇中核真名。特权模式目前有三种选择：
1. 使用申威26010式的主从模式，片上集成一个可运行操作系统的内核，在操作系统中能看到所有的物理编址，负责完成系统的配置和任务调度。在这种配置下，DSP内核本身不必拥有特权模式；
    2. 依然使用主从模式，类似于GPU的全被动启动方式，初始化由PCIe通过远端Host发送；
    3. 使用嵌入式/DSP的startup.c和初始化API流程，由DSP内核运行初始化程序完成初始化配置。这种选择下需要每个内核具有特权模式，且需要系统在特权模式下开启对L1I/D SPM别名的物理地址切换；

# 2 L1D Memory

申威26010和英伟达GPU采用的可配置L1D Memory的容量均为64KB，TI C66x CorePac允许最大128KB的L1D Memory配置 (具体容量取决于器件)，其他如ADI的SHARC内核甚至将L1D/I总容量加到了640KB，可见算力芯片相比通用处理器更注重降低缓存的容量缺失问题。L1D Memory总容量设置为64KB，其中Cache的最大容量为32KB，SPM部分最少32KB用于装填运行栈等动态数据。缓存按照常用的双路组联，缓存行容量为64B。替换策略采用LRU策略，每个索引需要一个LRU位用于记录哪个缓存行最近使用过。

TI TMS320C6678采用私有L1和L2 Cache，两级采用non-inclusive包含方式，其中L1 Cache是read-only-allocate，对于写缺失，TI选择使用write-through，不进行对L1的缓存行分配。这种方式严格遵循常规L1 Cache控制hit time和L2 Cache控制miss rate。但对于共享L2 Cache，为了降低频繁的写穿透至L2，考虑采用read-and-write-allocate的方式，结合ACE一致性保证多核数据一致性。

## 2.1 容量配置寄存器L1D_CACHE_VOL_CFG

缓存容量支持0KB, 4KB, 8KB, 16KB, 32KB，编码分别为000, 001, 010, 011, 100，其他编码均饱和在32KB。

## 2.2 组件说明

SRAM组织形式：内核中存在两条ld/st流水，在L1缓存响应时间严格小于一个时钟周期的前提下，应选用单端口SRAM。访存流水线中访存的最大粒度是word，最小粒度是byte，则SRAM字宽选为单字，分8个bank，每个bank深度为2KWord。数据采用地址交织并列排放在8个bank中。采用分立标签-状态位SRAM，每个SRAM的输出位宽为27Bit，取2或4个深度为256或64的双端口SRAM作为标签-状态位存储器，内容排放采用地址交织。

缓存控制器需要的功能：
1. 容量配置将地址低位的SRAM配置成Cache；
    2. 识别并处理非对齐访问；
    3. 对于发生行缺失的情况对外发起监听事务，在需要替换行的情况下驱逐缓存行；
    4. 对于外部监听事务，改写行状态，必要时写回脏数据；
    5. 完成启动段和行替换阶段的预取；
    6. 对本地L1 SPM段的访问：类似TI的方法，在全局地址段和虚拟地址段都提供64K的地址段作为编址别名，内核访问本地L1 SPM可以使用别名进行访问，访问簇内远端L1 SPM只能使用其虚拟地址空间编址访问；
    7. 对L1 Cache段的访问：TI使用VIVT缓存，这是因为其默认设置中所有可寻址内存 (包含所有SPM和2G DRAM，扩充DRAM使用其MSMC的MPAX寄存器配置address extension实现，类似于含MMU系统将一块2GB虚拟内存映射给不同的物理内存段)都被放在4GB空间内，其虚拟地址和物理地址转换仅使用位宽扩充，这使得TI的逻辑地址和物理地址完全对应，在后续编址中我们考虑使用类似的方法，将片上的SPM尽量压缩在32位内，仅通过位宽扩充完成虚实转换；对于DDR内存，采用段偏移方式实现虚实转换。这样一来简化了地址转换。

状态位+标签SRAM：
1. 采用VIPT，物理地址位宽32Bit，设置可配置缓存中标签宽度最大时在4KB 时取得，共20Bit；
2. 缓存行数量的上限在32KB时取得，共512行。加上valid, share, dirty三个行状态位，共需要(20+3)×512=11.5Kbit的标签-状态位SRAM;
3. 每个组固定一个LRU位用于确定最近一个使用的缓存行，共需32B的LRU寄存器堆。 (注：目前没有方法降低可配置存储的碎片化位容量)

# 3 L1I Memory

大多数DSP处理器同样SPM和Cache混合的L1I Memory。TI TMS320C6678八核DSP处理器使用最大容量32KB的可配容量比的L1I Memory。ADI的SHARC-FX处理器采用固定容量比的L1 Cache和L1 SPM，其中Cache占32KB而SPM占64KB。	项目中考虑采用48KB的可配置L1I Memory。

## 3.1 组联数设置

相比超标量处理器，VLIW处理器将发掘指令集并行能力的工作和部分数据性依赖的工作交给编译器提前处理，而非在运行时实时处理。指令缓存中的指令粒度也非常规顺序和超标量处理器中的单条指令，而是以指令包的形式存在。这对缓存的容量和组联数做出了巨大的挑战。

ADI SHARC-FX采用四路VLIW，其L1I Cache采用四路组联，缓存行大小64B；ADI SHARC+是非VLIW架构， L1I Cache采用两路组联,，缓存行大小64B；TI TMS320C6678的内核是八路VLIW，L1I Cache采用直接映射，缓存行大小为32B。因为我们的处理器也采用的是八路VLIW，所以考虑使用TI的直接映射策略，即使用非常规的32B缓存行容量 (只容纳一个八指令包)。大路数VLIW使用直接映射缓存一定程度上减轻了缓存的容量压力。

## 3.2 容量配置寄存器L1I_CACHE_VOL_CFG

与L1D Memory类似，L1I_CACHE_VOL_CFG也有八种配置：000, 001, 010, 011, 100, 分别对应0K (全SPM), 4K, 8K, 16K, 32K。其余高于100的编码饱和在32KB。

## 3.3 组件说明

SRAM组织形式：取值周期为单周期，选用单口SRAM；bank设置固定为32字宽，2KWord深度，共8个bank。标签-状态位存储器数据位宽27Bit，总容量27Kbits，选用4个256深度的双端bank，将内容地址交织存放在四个bank macro中。

缓存控制器需要的功能：
1. 根据容量配置，对于可缓存地址访问缓存，其他在L1I地址段内的部分访问SPM；
2.  完成缓存行替代，被替代的缓存行不被写回；
3.  完成启动阶段和分支阶段的预取
4.  与L1D Cache一样，同样使用VIPT

# 4 L2 Memory

TI TMS320C6678八个内核的私有L2缓存总共4MB，另有4MB的共享L2 SPM；Cadence HIFI-5s SMP的L2总容量最大支持8MB容量。TI没有维护硬件多核缓存一致性，所以数据管理高度依赖软件和操作系统管理，在多个核心都含有单独的数据备份情况下有效容量降低。我们和Cadence HIFI-5s都使用ACE信号扩展维护硬件一致性，即二级缓存中只存在一个数据副本，这样对二级缓存的容量可以适度降低，暂时容量定为4MB。L2组联路数为4，行容量128B。指令固定占用前两行，数据四行都可以占用，以控制指令/数据容量比。

## 4.1 SRAM组织形式
## 4.2 缓存控制器
## 4.3 容量配置寄存器
